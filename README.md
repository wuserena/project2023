# Comparison and Analysis of Vision Transformer Variants
Last year, ChatGPT successfully captured public attention, and now we are anticipating the arrival of the next generation. Our research analyzes two Transformer-based backbones, Vision Transformer and Swin Transformer, o identify the optimal architecture for computer vision tasks. We aim to demonstrate how Transformers can be applied to vision problems and to understand the reasoning behind the networkâ€™s decisions.

## Project Page
Please visit our project page! It documents our learning process, the challenges we encountered, and how we addressed them. We hope you enjoy exploring it.
[Comparison and Analysis of Vision Transformer Variants](https://sites.google.com/view/project666/home)

## System Environment  
   * python 3.9
   * Pytorch 2.0.1
   * CUDA 11.7

* Swin Transformer
  * microsoft: https://github.com/microsoft/Swin-Transformer
  * original code: https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_classification/swin_transformer
* Vision Transformer
  * original code: https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_classification/vision_transformer
  * early stopping code from: https://github.com/Bjarten/early-stopping-pytorch/tree/master
* GradCam
   * original code: https://github.com/WZMIAOMIAO/deep-learning-for-image-processing/tree/master/pytorch_classification/grad_cam
* SorceCam
   * original code: https://github.com/jacobgil/pytorch-grad-cam/blob/master/tutorials/vision_transformers.md


